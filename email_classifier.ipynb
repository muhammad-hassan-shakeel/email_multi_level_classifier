{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AppGallery.csv\")\n",
    "\n",
    "# convert the dtype object to unicode string\n",
    "df['Interaction content'] = df['Interaction content'].values.astype('U')\n",
    "df['Ticket Summary'] = df['Ticket Summary'].values.astype('U')\n",
    "\n",
    "#Optional: rename variable names for remebering easily\n",
    "df[\"y1\"] = df[\"Type 1\"]\n",
    "df[\"y2\"] = df[\"Type 2\"]\n",
    "df[\"y3\"] = df[\"Type 3\"]\n",
    "df[\"y4\"] = df[\"Type 4\"]\n",
    "df[\"x\"] = df['Interaction content']\n",
    "\n",
    "df[\"y\"] = df[\"y2\"]\n",
    "\n",
    "# remove empty y\n",
    "df = df.loc[(df[\"y\"] != '') & (~df[\"y\"].isna()),]\n",
    "df.shape\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "from transformers import pipeline\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "\n",
    "def trans_to_en(texts):\n",
    "    t2t_m = \"facebook/m2m100_418M\"\n",
    "    t2t_pipe = pipeline(task='text2text-generation', model=t2t_m)\n",
    "\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(t2t_m)\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(t2t_m)\n",
    "    nlp_stanza = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\",\n",
    "                                 download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "\n",
    "    text_en_l = []\n",
    "    for text in texts:\n",
    "        if text == \"\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "            continue\n",
    "\n",
    "        doc = nlp_stanza(text)\n",
    "        print(doc.lang)\n",
    "        if doc.lang == \"en\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "        else:\n",
    "            lang = doc.lang\n",
    "            if lang == \"fro\":  # fro = Old French\n",
    "                lang = \"fr\"\n",
    "            elif lang == \"la\":  # latin\n",
    "                lang = \"it\"\n",
    "            elif lang == \"nn\":  # Norwegian (Nynorsk)\n",
    "                lang = \"no\"\n",
    "            elif lang == \"kmr\":  # Kurmanji\n",
    "                lang = \"tr\"\n",
    "\n",
    "            case = 2\n",
    "\n",
    "            if case == 1:\n",
    "                text_en = t2t_pipe(text, forced_bos_token_id=t2t_pipe.tokenizer.get_lang_id(lang='en'))\n",
    "                text_en = text_en[0]['generated_text']\n",
    "            elif case == 2:\n",
    "                tokenizer.src_lang = lang\n",
    "                encoded_hi = tokenizer(text, return_tensors=\"pt\")\n",
    "                generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "                text_en = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                text_en = text_en[0]\n",
    "            else:\n",
    "                text_en = text\n",
    "\n",
    "            text_en_l = text_en_l + [text_en]\n",
    "\n",
    "            print(text)\n",
    "            print(text_en)\n",
    "\n",
    "    return text_en_l\n",
    "\t\n",
    "#Calling translation method\n",
    "# Note that the we can only translate a limited number of words so we are only translating ticket summary and not interaction content\n",
    "\n",
    "temp[\"ts_en\"] = trans_to_en(temp[\"ts\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=4, max_df=0.90)\n",
    "x1 = tfidfconverter.fit_transform(temp[\"Interaction content\"]).toarray()\n",
    "x2 = tfidfconverter.fit_transform(temp[\"ts_en\"]).toarray()\n",
    "X = np.concatenate((x1, x2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = temp.y.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad test cases from test dataset\n",
    "Test_size = 0.20\n",
    "y_series = pd.Series(y)\n",
    "good_y_value = y_series.value_counts()[y_series.value_counts() >= 3].index\n",
    "y_good = y[y_series.isin(good_y_value)]\n",
    "X_good = X[y_series.isin(good_y_value)]\n",
    "y_bad = y[y_series.isin(good_y_value) == False]\n",
    "X_bad = X[y_series.isin(good_y_value) == False]\n",
    "test_size = X.shape[0] * 0.2 / X_good.shape[0]\n",
    "print(f\"new_test_size: {new_test_size}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_good, y_good,     test_size=test_size, random_state=0)\n",
    "X_train = np.concatenate((X_train, X_bad), axis=0)\n",
    "y_train = np.concatenate((y_train, y_bad), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df\n",
    "# remove re:\n",
    "# remove extrac white space\n",
    "# remove\n",
    "noise = \"(sv\\s*:)|(wg\\s*:)|(ynt\\s*:)|(fw(d)?\\s*:)|(r\\s*:)|(re\\s*:)|(\\[|\\])|(aspiegel support issue submit)|(null)|(nan)|((bonus place my )?support.pt 自动回复:)\"\n",
    "temp[\"ts\"] = temp[\"Ticket Summary\"].str.lower().replace(noise, \" \", regex=True).replace(r'\\s+', ' ',\n",
    "                                                                                        regex=True).str.strip()\n",
    "temp_debug = temp.loc[:, [\"Ticket Summary\", \"ts\", \"y\"]]\n",
    "\n",
    "temp[\"ic\"] = temp[\"Interaction content\"].str.lower()\n",
    "noise_1 = [\n",
    "    \"(from :)|(subject :)|(sent :)|(r\\s*:)|(re\\s*:)\",\n",
    "    \"(january|february|march|april|may|june|july|august|september|october|november|december)\",\n",
    "    \"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\",\n",
    "    \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\",\n",
    "    \"\\d{2}(:|.)\\d{2}\",\n",
    "    \"(xxxxx@xxxx\\.com)|(\\*{5}\\([a-z]+\\))\",\n",
    "    \"dear ((customer)|(user))\",\n",
    "    \"dear\",\n",
    "    \"(hello)|(hallo)|(hi )|(hi there)\",\n",
    "    \"good morning\",\n",
    "    \"thank you for your patience ((during (our)? investigation)|(and cooperation))?\",\n",
    "    \"thank you for contacting us\",\n",
    "    \"thank you for your availability\",\n",
    "    \"thank you for providing us this information\",\n",
    "    \"thank you for contacting\",\n",
    "    \"thank you for reaching us (back)?\",\n",
    "    \"thank you for patience\",\n",
    "    \"thank you for (your)? reply\",\n",
    "    \"thank you for (your)? response\",\n",
    "    \"thank you for (your)? cooperation\",\n",
    "    \"thank you for providing us with more information\",\n",
    "    \"thank you very kindly\",\n",
    "    \"thank you( very much)?\",\n",
    "    \"i would like to follow up on the case you raised on the date\",\n",
    "    \"i will do my very best to assist you\"\n",
    "    \"in order to give you the best solution\",\n",
    "    \"could you please clarify your request with following information:\"\n",
    "    \"in this matter\",\n",
    "    \"we hope you(( are)|('re)) doing ((fine)|(well))\",\n",
    "    \"i would like to follow up on the case you raised on\",\n",
    "    \"we apologize for the inconvenience\",\n",
    "    \"sent from my huawei (cell )?phone\",\n",
    "    \"original message\",\n",
    "    \"customer support team\",\n",
    "    \"(aspiegel )?se is a company incorporated under the laws of ireland with its headquarters in dublin, ireland.\",\n",
    "    \"(aspiegel )?se is the provider of huawei mobile services to huawei and honor device owners in\",\n",
    "    \"canada, australia, new zealand and other countries\",\n",
    "    \"\\d+\",\n",
    "    \"[^0-9a-zA-Z]+\",\n",
    "    \"(\\s|^).(\\s|$)\"]\n",
    "for noise in noise_1:\n",
    "    print(noise)\n",
    "    temp[\"ic\"] = temp[\"ic\"].replace(noise, \" \", regex=True)\n",
    "temp[\"ic\"] = temp[\"ic\"].replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "temp_debug = temp.loc[:, [\"Interaction content\", \"ic\", \"y\"]]\n",
    "\n",
    "print(temp.y1.value_counts())\n",
    "good_y1 = temp.y1.value_counts()[temp.y1.value_counts() > 10].index\n",
    "temp = temp.loc[temp.y1.isin(good_y1)]\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "p_result = pd.DataFrame(classifier.predict_proba(X_test))\n",
    "p_result.columns = classifier.classes_\n",
    "print(p_result)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
