{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AppGallery.csv\")\n",
    "\n",
    "# convert the dtype object to unicode string\n",
    "df['Interaction content'] = df['Interaction content'].values.astype('U')\n",
    "df['Ticket Summary'] = df['Ticket Summary'].values.astype('U')\n",
    "\n",
    "#Optional: rename variable names for remebering easily\n",
    "df[\"y1\"] = df[\"Type 1\"]\n",
    "df[\"y2\"] = df[\"Type 2\"]\n",
    "df[\"y3\"] = df[\"Type 3\"]\n",
    "df[\"y4\"] = df[\"Type 4\"]\n",
    "df[\"x\"] = df['Interaction content']\n",
    "\n",
    "df[\"y\"] = df[\"y2\"]\n",
    "\n",
    "# remove empty y\n",
    "df = df.loc[(df[\"y\"] != '') & (~df[\"y\"].isna()),]\n",
    "df.shape\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "from transformers import pipeline\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "\n",
    "def trans_to_en(texts):\n",
    "    t2t_m = \"facebook/m2m100_418M\"\n",
    "    t2t_pipe = pipeline(task='text2text-generation', model=t2t_m)\n",
    "\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(t2t_m)\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(t2t_m)\n",
    "    nlp_stanza = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\",\n",
    "                                 download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "\n",
    "    text_en_l = []\n",
    "    for text in texts:\n",
    "        if text == \"\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "            continue\n",
    "\n",
    "        doc = nlp_stanza(text)\n",
    "        print(doc.lang)\n",
    "        if doc.lang == \"en\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "        else:\n",
    "            lang = doc.lang\n",
    "            if lang == \"fro\":  # fro = Old French\n",
    "                lang = \"fr\"\n",
    "            elif lang == \"la\":  # latin\n",
    "                lang = \"it\"\n",
    "            elif lang == \"nn\":  # Norwegian (Nynorsk)\n",
    "                lang = \"no\"\n",
    "            elif lang == \"kmr\":  # Kurmanji\n",
    "                lang = \"tr\"\n",
    "\n",
    "            case = 2\n",
    "\n",
    "            if case == 1:\n",
    "                text_en = t2t_pipe(text, forced_bos_token_id=t2t_pipe.tokenizer.get_lang_id(lang='en'))\n",
    "                text_en = text_en[0]['generated_text']\n",
    "            elif case == 2:\n",
    "                tokenizer.src_lang = lang\n",
    "                encoded_hi = tokenizer(text, return_tensors=\"pt\")\n",
    "                generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "                text_en = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                text_en = text_en[0]\n",
    "            else:\n",
    "                text_en = text\n",
    "\n",
    "            text_en_l = text_en_l + [text_en]\n",
    "\n",
    "            print(text)\n",
    "            print(text_en)\n",
    "\n",
    "    return text_en_l\n",
    "\t\n",
    "#Calling translation method\n",
    "# Note that the we can only translate a limited number of words so we are only translating ticket summary and not interaction content\n",
    "\n",
    "temp[\"ts_en\"] = trans_to_en(temp[\"ts\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=4, max_df=0.90)\n",
    "x1 = tfidfconverter.fit_transform(temp[\"Interaction content\"]).toarray()\n",
    "x2 = tfidfconverter.fit_transform(temp[\"ts_en\"]).toarray()\n",
    "X = np.concatenate((x1, x2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
